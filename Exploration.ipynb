{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/mohamedkhodeir/.cache/torch/hub/huggingface_pytorch-pretrained-BERT_master\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n",
      "100%|██████████| 213450/213450 [00:03<00:00, 58028.97B/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-pretrained-BERT', 'bertTokenizer', 'bert-base-cased', do_basic_tokenize=False)\n",
    "\n",
    "# Tokenized input\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'Who',\n",
       " 'was',\n",
       " 'Jim',\n",
       " 'He',\n",
       " '##nson',\n",
       " '?',\n",
       " '[SEP]',\n",
       " 'Jim',\n",
       " 'He',\n",
       " '##nson',\n",
       " 'was',\n",
       " 'a',\n",
       " 'puppet',\n",
       " '##eer',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/mohamedkhodeir/.cache/torch/hub/huggingface_pytorch-pretrained-BERT_master\n",
      "100%|██████████| 313/313 [00:00<00:00, 124946.91B/s]\n",
      "100%|██████████| 435779157/435779157 [02:06<00:00, 3458177.72B/s]\n"
     ]
    }
   ],
   "source": [
    "### Get the hidden states computed by `bertModel`\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "model = torch.hub.load('huggingface/pytorch-pretrained-BERT', 'bertModel', 'bert-base-cased')\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0671, -0.0819, -0.3849,  ...,  0.2479,  0.8418,  0.5378],\n",
       "         [-0.0828, -0.2384,  0.3622,  ..., -0.0386, -0.5637,  0.1459],\n",
       "         [ 0.0357, -0.1931,  0.3957,  ...,  0.4133, -0.1263, -0.0303],\n",
       "         ...,\n",
       "         [ 0.1778,  0.0921, -0.0885,  ...,  0.6259, -0.3713,  0.0748],\n",
       "         [ 0.0062, -0.4483, -0.4176,  ...,  0.1641, -0.1112, -0.0908],\n",
       "         [ 0.3182, -0.0281, -0.4652,  ...,  0.5634,  1.3185,  0.6513]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = torch.randn((10, 32, 200))\n",
    "document = torch.randn((500, 32, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_matrix = torch.randn(200, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_con = torch.matmul(document, context_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.uint8)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_con[0,0] == context_matrix.mv(document[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200, 200])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_matrix.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor to have size 200 at dimension 1, but got size 16000 for argument #2 'batch2' (while checking arguments for bmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-681c9e48518c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc_con\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor to have size 200 at dimension 1, but got size 16000 for argument #2 'batch2' (while checking arguments for bmm)"
     ]
    }
   ],
   "source": [
    "doc_con = torch.matmul(context_matrix, document.view(-1, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_con = torch.einsum('qh,mnh->mnq',context_matrix, document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1444e-05)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(context_matrix.mv(document[0,3]) - doc_con[0,3]).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 200]), torch.Size([500, 32, 200]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question[0].shape, doc_con.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.einsum('nh,mnh->mn',question[0], doc_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 32])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32, 200])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2000])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question.transpose(0, 1).reshape(32, 2000).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.1035e-05)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0,1] - question[0,1].dot(doc_con[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 10, 200]), torch.Size([32, 200, 500]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question.transpose(0, 1).shape, doc_con.permute(1, 2, 0).shape\n",
    "#.bmm(doc_con.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question.transpose(0, 1).bmm(doc_con.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class TaskSpecificAttention(nn.Module):\n",
    "\tdef __init__(self, input_size, projection_size):\n",
    "\t\tsuper(TaskSpecificAttention, self).__init__()\n",
    "\t\tself.input_size = input_size\n",
    "\t\tself.projection_size = projection_size\n",
    "\t\tself.context_vector = torch.randn((1, 1, projection_size), requires_grad=True)\n",
    "\t\tself.input_projection = nn.Tanh(nn.Linear(input_size, projection_size))\n",
    "\t\tself.softmax = nn.Softmax()\n",
    "\n",
    "\tdef forward(self, input_seq):\n",
    "\t\t'''inputs should be [seq_length, batch_size, input_size]'''\n",
    "\t\tvector_attention = self.input_projection(input_seq) # should be [seq_length, batch_size, output_size]\n",
    "\t\tattention_weights = self.softmax((vector_attention * self.context_vector).sum(2, keepdim=True), dim=0) # should be [seq_length, batch_size, 1]\n",
    "\t\treturn attention_weights\n",
    "\n",
    "class BiLinearAttention(nn.Module):\n",
    "\tdef __init__(self, input_size):\n",
    "\t\tsuper(BiLinearAttention, self).__init__()\n",
    "\t\tself.input_size = input_size\n",
    "\n",
    "\t\tself.context_matrix = torch.randn((input_size, input_size), requires_grad=True)\n",
    "\t\tself.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "\tdef forward(self, question, document):\n",
    "\t\t'''inputs should be [seq_length, batch_size, input_size]'''\n",
    "\t\tdocument_context = torch.matmul(document, self.context_matrix)\n",
    "\t\tattention_matrix = question.transpose(0, 1).bmm(document_context.permute(1, 2, 0))\n",
    "\t\tattention_weights = self.softmax(attention_matrix.sum(1, keepdim=True).permute(2,0,1))\n",
    "\t\treturn attention_weights\n",
    "\n",
    "\n",
    "class AttentiveReader(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, lstm_layers=1, lstm_bidirectional=True):\n",
    "        super(AttentiveReader, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.question_lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=lstm_layers, bidirectional=lstm_bidirectional)\n",
    "        self.document_lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=lstm_layers, bidirectional=lstm_bidirectional)\n",
    "        self.attention = BiLinearAttention(input_size=hidden_dim * 2 if lstm_bidirectional else 1)\n",
    "\n",
    "    def forward(self, question, document):\n",
    "        question_embedding = self.word_embeddings(question)\n",
    "        document_embedding = self.word_embeddings(document)\n",
    "        print(question_embedding.shape)\n",
    "\n",
    "        question_encoding, _ = self.question_lstm(question_embedding)\n",
    "        document_encoding, _ = self.document_lstm(document_embedding)\n",
    "\n",
    "        attention = self.attention(question_encoding, document_encoding)\n",
    "        print(attention.shape, document_encoding.shape)\n",
    "        output = (attention * document_encoding).sum(2)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = AttentiveReader(100, 100, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question = torch.randint(low=0, high=50000, size=((10,32)))\n",
    "document = torch.randint(low=0, high=50000, size=((200,32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 100])\n",
      "torch.Size([200, 32, 1]) torch.Size([200, 32, 200])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 32])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x(question, document).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/16/4d247e27c55a7b6412e7c4c86f2500ae61afcbf5932b9e3491f8462f8d9e/nltk-3.4.4.zip (1.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.5MB 170kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/mohamedkhodeir/anaconda3/envs/deeprl/lib/python3.5/site-packages (from nltk) (1.11.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Running setup.py bdist_wheel for nltk ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/mohamedkhodeir/Library/Caches/pip/wheels/41/c8/31/48ace4468e236e0e8435f30d33e43df48594e4d53e367cf061\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.4.4\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mohamedkhodeir/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'work', 'and', 'no', 'play', 'makes', 'jack', 'a', 'dull', 'boy', ',', 'all', 'work', 'and', 'no', 'play']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "data = \"All work and no play makes jack a dull boy, all work and no play\"\n",
    "print(word_tokenize(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "model = loadGloveModel('data/glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed = nn.Embedding(50000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 100])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?',\n",
    " ]\n",
    "def corpus_iterator():\n",
    "    for i in corpus:\n",
    "        yield i\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus_iterator())\n",
    "print(vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = pickle.load(open('data/count_vectorizer.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zubaydah'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.get_feature_names()[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.build_vocab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# glove_embeddings = load_glove_model()\n",
    "data_vocab = load_count_vectorizer_data_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = data_vocab['vectorizer']\n",
    "counts = data_vocab['counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "counts = vectorizer.fit_transform(data_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['the', 'of', 'to', 'and', 'in', 'that', 'for', 'is', 'on', 'it',\n",
       "       'with', 'as', 'by', 'this', 'was', 'have', 'are', 'be', 'we',\n",
       "       'not', 'or', 'from', 'at', 'you', 'he', 'an', 'but', 'they', 'has',\n",
       "       'said', 'who', 'about', 'more', 'his', 'all', 'would', 'will',\n",
       "       'their', 'were', 'one', 'which', 'people', 'what', 'been', 'there',\n",
       "       'our', 'if', 'state', 'other', 'than', 'so', 'had', 'can', 'trump',\n",
       "       'no', 'when', 'president', 'also', 'new', 'do', 'out', 'up',\n",
       "       'year', 'states', 'any', 'time', 'some', 'after', 'over',\n",
       "       'because', 'years', 'its', 'these', 'those', 'may', 're', 'she',\n",
       "       'them', 'just', 'first', 'into', 'now', 'two', 'only', 'percent',\n",
       "       'under', 'law', 'such', 'how', 'like', '000', 'most', 'united',\n",
       "       'her', 'going', 'health', 'government', 'many', 'know', 'could',\n",
       "       'tax', 'us', 'well', 'think', 'very', 'public', 'get', 'country',\n",
       "       'make', 'did', 'federal', 'house', 'national', 'your', 'should',\n",
       "       'before', 'don', 'made', 'even', 'where', '10', 'through',\n",
       "       'clinton', 'million', 'right', 'american', 'say', 'while', 'my',\n",
       "       'here', 'last', 'during', 'then', 'including', 'work', 'see',\n",
       "       'use', 'being', 'obama', 'back', 'number', 'against',\n",
       "       'information', 'report', 'news', 'bill', 'much', 'way', 'since',\n",
       "       'him', 'between', 'me', 'want', 'care', 'act', 'department',\n",
       "       'take', 'security', 'part', '2016', 've', 'world', 'both', 'same',\n",
       "       'support', 'day', 'office', 'used', 'campaign', 'does', 'three',\n",
       "       'school', 'need', 'go', 'fact', 'high', 'says', 'data', 'based',\n",
       "       'senate', 'policy', 'according', 'congress', 'down', 'general',\n",
       "       'program', 'plan', 'long', 'children', '11', 'white', 'every',\n",
       "       'court', '12', 'told', 'each', 'system', 'administration',\n",
       "       'another', '20', 'still', 'republican', 'income', 'good', '15',\n",
       "       'washington', 'political', 'case', 'america', 'end', 'billion',\n",
       "       'today', 'why', 'without', '2015', '2017', 'whether', 'committee',\n",
       "       'group', 'however', 'mr', 'money', 'family', 'found', 'donald',\n",
       "       'election', 'secretary', 'called', 'look', 'business', '30',\n",
       "       'rate', 'change', 'person', 'come', 'must', 'help', 'services',\n",
       "       'times', 'less', 'women', 'party', 'among', 'immigration', 'pay',\n",
       "       '2014', 'members', 'own', 'let', 'americans', 'never', 'city',\n",
       "       'off', 'great', 'former', 'question', 'economic', 'increase',\n",
       "       'provide', 'vote', 'jobs', 'lot', 'next', 'york', 'order',\n",
       "       'around', 'got', 'really', 'local', 'point', '13', 'issue', 'life',\n",
       "       'budget', 'shall', '18', 'week', 'insurance', 'believe',\n",
       "       'important', 'total', 'again', 'war', 'research', 'put', 'four',\n",
       "       'service', '25', 'place', 'days', 'social', 'several', 'education',\n",
       "       'least', 'section', 'home', 'countries', 'different', '2018',\n",
       "       'working', 'things', 'university', 'second', 'current', 'didn',\n",
       "       'military', 'cost', 'police', 'foreign', '2013', 'saying', 'too',\n",
       "       'border', 'within', 'officials', 'media', 'making', 'per', '14',\n",
       "       'democratic', 'reported', 'll', 'something', 'later', '16',\n",
       "       'democrats', 'far', 'asked', 'better', 'months', 'recent', 'post',\n",
       "       'statement', 'job', 'using', '50', 'top', 'full', 'center',\n",
       "       'evidence', 'available', 'across', 'show', '2012', '100', 'term',\n",
       "       'republicans', '17', 'rights', 'following', 'senator', 'small',\n",
       "       'doing', 'coverage', 'actually', 'level', 'community', 'justice',\n",
       "       'process', 'private', 'little', 'few', 'individual', 'five',\n",
       "       'done', 'old', 'story', 'related', 'county', 'real', 'march',\n",
       "       'north', 'claim', 'power', 'higher', 'might', 'already',\n",
       "       'presidential', 'control', 'john', 'international', 'took', 'non',\n",
       "       'legal', 'florida', 'past', 'south', 'spending', 'until', 'taxes',\n",
       "       'director', 'provided', 'set', 'action', 'nation', 'history',\n",
       "       'give', 'texas', 'likely', 'include', 'example', 'russia', 'came',\n",
       "       'given', 'issues', 'hillary', 'able', 'means', 'find', 'period',\n",
       "       'economy', 'big', 'others', 'district', 'workers', 'deal', 'keep',\n",
       "       'human', 'families', 'population', 'trade', 'company', 'governor',\n",
       "       'average', 'free', 'continue', 'clear', 'though', 'left', 'doesn',\n",
       "       'press', 'large', 'thing', 'mean', 'cases', 'major', 'age',\n",
       "       'plans', 'medical', 'energy', 'month', 'laws', 'having', 'force',\n",
       "       'best', 'enforcement', 'future', '2010', 'man', 'low', '19',\n",
       "       'rates', 'share', 'benefits', 'costs', 'programs', 'early',\n",
       "       'access', 'june', '21', 'january', 'special', 'july',\n",
       "       'legislation', 'known', 'growth', '2011', '24', 'article',\n",
       "       'immigrants', 'call', 'additional', 'individuals', 'received',\n",
       "       'changes', 'agency', 'name', 'check', 'nearly', 'financial',\n",
       "       'ever', 'went', 'death', 'amount', 'taken', 'yet', 'thank', 'com',\n",
       "       'russian', 'read', 'market', 'date', 'problem', 'child',\n",
       "       'decision', 'record', 'often', 'reports', 'further', 'crime'],\n",
       "      dtype='<U3843')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.asarray(counts.sum(0)).flatten()/tfidf.idf_\n",
    "tfidf_sorting = np.argsort(x)[::-1]\n",
    "feature_array[tfidf_sorting[:limit]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer().fit(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = tfidf.transform(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_array = np.array(count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365905"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93235, 365905)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_sorting = np.argsort(np.asarray(response.sum(0))).flatten()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "limit = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['the', 'to', 'of', 'and', 'in', 'that', 'for', 'is', 'on', 'it',\n",
       "       'he', 'was', 'as', 'with', 'trump', 'we', 'are', 'by', 'said',\n",
       "       'have', 'you', 'this', 'be', 'from', 'not', 'at', 'or', 'his',\n",
       "       'has', 'they', 'an', 'but', 'who', 'will', 'would', 'state',\n",
       "       'about', 'our', 'more', 'president', 'tax', 'their', 'people',\n",
       "       'were', 'she', 'all', 'percent', 'health', 'new', 'had', 'than',\n",
       "       'clinton', 'one', 'been', 'states', 'her', 'obama', 'there', 'if',\n",
       "       'what', 'year', 'can', 'which', 'no', 'when', 'law', 'so', 'up',\n",
       "       'out', 'other', 'also', '000', 'do', 'bill', 'says', 'million',\n",
       "       'after', 'years', 'its', 'house', 'your', 'government', 'over',\n",
       "       'federal', 'united', 'time', 'any', 'some', 'care', 're',\n",
       "       'because', 'us', 'these', 'just', 'first', 'those', 'them', 'news',\n",
       "       'may', 'country', 'public', 'now', 'like', 'into', '2016',\n",
       "       'national', 'american', 'two', 'senate', 'only', 'campaign',\n",
       "       'school', 'under', 'most', 'my', 'how', 'could', 'get', 'going',\n",
       "       'many', 'immigration', 'billion', 'children', 'data', 'know',\n",
       "       'security', 'report', 'last', 'donald', 'white', 'did',\n",
       "       'department', 'don', 'border', 'information', 'such', 'florida',\n",
       "       'during', 'him', 'world', 'should', 'court', 'act', 'against',\n",
       "       'make', 'think', 'even', 'election', 'women', 'very', 'republican',\n",
       "       'insurance', 'while', 'before', 'work', 'plan', 'police', '2017',\n",
       "       'right', '10', 'texas', 'made', 'here', 'say', 'income',\n",
       "       'congress', 'see', 'through', 'policy', 'budget', 'use', 'since',\n",
       "       'office', 'vote', 'number', 'jobs', 'where', 'administration',\n",
       "       'mr', 'program', 'day', 'support', 'well', 'being', 'rate', 'me',\n",
       "       'back', 'according', 'including', 'americans', 'city', 'america',\n",
       "       'then', '2015', '2018', 'committee', 'told', 'russia', 'hillary',\n",
       "       've', 'want', 'washington', 'gun', 'money', 'education',\n",
       "       'democrats', 'much', 'russian', 'pay', 'way', 'fact', 'between',\n",
       "       'system', 'take', 'every', 'political', 'high', 'down', 'county',\n",
       "       'family', 'used', 'group', 'party', 'today', 'three', 'university',\n",
       "       'immigrants', 'same', 'part', 'military', 'republicans',\n",
       "       'coverage', '2014', 'media', 'economic', 'general', 'social',\n",
       "       'need', 'democratic', 'taxes', 'go', 'does', 'former', 'based',\n",
       "       'both', 'york', 'business', 'governor', 'research', 'services',\n",
       "       'found', 'war', 'spending', 'members', 'secretary', 'times',\n",
       "       'still', 'south', 'officials', 'presidential', 'students', 'long',\n",
       "       'help', 'change', 'countries', '11', 'local', 'increase', 'called',\n",
       "       'north', 'case', 'week', 'each', 'statement', 'cost', '2013',\n",
       "       'rights', 'per', '12', '20', 'illegal', 'life', 'home', 'off',\n",
       "       'company', '15', 'another', 'never', 'center', 'energy', 'without',\n",
       "       'story', 'medicaid', 'com', 'must', 'good', 'workers', 'justice',\n",
       "       'why', 'service', 'california', 'district', 'own', 'foreign',\n",
       "       'trade', 'order', 'post', 'water', 'end', 'man', '30', 'crime',\n",
       "       'march', 'among', 'claim', 'families', 'voters', 'average', 'show',\n",
       "       'wisconsin', 'africa', 'check', 'nation', 'come', 'however',\n",
       "       'less', '2012', 'old', 'reported', 'population', 'history',\n",
       "       'around', 'whether', 'funding', 'child', 'bush', 'free', 'plans',\n",
       "       'obamacare', 'john', 'scott', 'total', 'next', 'job', 'person',\n",
       "       'medicare', 'legislation', 'private', 'site', 'working', 'great',\n",
       "       'four', 'international', 'black', 'economy', 'legal', 'senator',\n",
       "       'medical', 'schools', 'deal', 'look', 'mexico', 'sen', 'current',\n",
       "       'abortion', 'enforcement', 'days', 'director', 'growth', 'real',\n",
       "       'fbi', 'top', 'laws', 'got', 'didn', 'recent', 'community',\n",
       "       'sanders', 'death', 'candidate', 'human', 'rates', 'several',\n",
       "       'provide', 'climate', 'issue', 'press', 'gov', 'facebook',\n",
       "       'across', 'saying', 'asked', 'costs', 'really', 'put', 'wall',\n",
       "       'email', 'drug', '100', 'power', 'full', 'example', 'month',\n",
       "       'article', 'iraq', 'second', 'believe', 'shows', 'place', 'lot',\n",
       "       'benefits', 'companies', 'amendment', 'too', 'five', 'claims',\n",
       "       'food', 'higher', 'executive', 'using', 'available', 'let',\n",
       "       'attorney', 'twitter', 'evidence', 'force', 'least', 'reform',\n",
       "       'iran', '50', 'again', 'global', 'already', 'college', 'video',\n",
       "       'access', 'control', 'defense', 'study', 'market', 'took',\n",
       "       'making', 'shall', 'months', 'non', 'far', 'nuclear', 'cut',\n",
       "       'july', '13', '2010', 'released', 'point', 'better', 'related',\n",
       "       'actually', 'criminal', 'june', 'individual', 'll', 'share', 'age',\n",
       "       'oil', 'past', 'read', '25', '18', 'small', 'nearly', 'voter',\n",
       "       'men', 'voted', 'find', 'term', 'agency', 'financial', 'something',\n",
       "       'programs', 'process', 'january', 'likely', 'violence', '2011',\n",
       "       'few', 'keep', 'reports', 'china', 'april', 'action', 'question',\n",
       "       'issues', 'rep', 'things', 'page', 'big', 'later', 'following',\n",
       "       'woman', 'citizens', 'came'], dtype='<U3843')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_array[tfidf_sorting[:limit]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['critiquait', 'tirage', 'déplacés', 'démocratiques', 'neutre',\n",
       "       'augmen', 'démontre', 'chiffre', 'dirais', 'soyons', 'démontrent',\n",
       "       'alarme', 'chiffré', 'chiffrées', 'souviens', 'délicatesse',\n",
       "       'pâturer', 'raffinage', 'dégoûtés', 'diminue', 'diminuer', 'misé',\n",
       "       'rouvert', 'rouvrir', 'qualitatif', 'pêcheries', 'amortit',\n",
       "       'faillites', 'pérennes', 'pénaliser', 'délicat', 'faible',\n",
       "       'délicate', 'fleuve', 'continuellement', 'renié', 'favorisions',\n",
       "       'habiller', 'importerait', 'instabilité', 'relancer', 'concrète',\n",
       "       'dépassé', 'rejette', 'habille', 'vrac', 'favoritisme', 'sinécure',\n",
       "       'souffle', 'concrets', 'autochtone', 'catégorie', 'solide',\n",
       "       'réformiste', 'déchirer', 'référendaires', 'passés', 'écologistes',\n",
       "       'écosystèmes', 'écologique', 'désolant', 'diffuseurs',\n",
       "       'réductions', 'autochtones', 'achats', 'amenée', 'fassent',\n",
       "       'amené', '050217', 'vare', 'enfocrment', 'ecpd', 'fods',\n",
       "       'addtional', 'sherrifs', 'criaghead', 'cnext', 'wonglist', '247ns',\n",
       "       'leift', '00010270', '00053669', '00053671', '00053700',\n",
       "       '00053962', '07940', '00010271', '8620', '00011171', '00010263',\n",
       "       '00054061', 'warndof', '00060783', '00055104', '00055588',\n",
       "       '00053597', '00060815', '00053522', '8769', '00060992', '02887',\n",
       "       '00052406', 'univest', '87123', '00011645', '00060966', '00053277',\n",
       "       '00060895', '00060816', '00053420', '37027', '00053423',\n",
       "       '00053424', '00053487', '00055739', 'agather', '36607',\n",
       "       'greenhunter', '00057423', '00057764', '00060358', '00010107',\n",
       "       '3417', '00060226', 'evercare', '00059720', '34306', '0562',\n",
       "       '00058217', 'whq', '00058632', '00059355', '00059266', '00051209',\n",
       "       '00057030', '00060700', 'talerico', '00010261', '00060697',\n",
       "       '00056026', '8662', 'tramuto', 'glenlake', '00060646', '00056279',\n",
       "       '00060540', '00056323', '00056413', '00056569', 'tredway', '8753',\n",
       "       '0368', '00051792', 'nastional', 'tonn', '00014952', '12031',\n",
       "       '00014874', '00029923', 'sandoloski', '00030746', '00030952',\n",
       "       '00030963', '00014618', 'dkm', 'filmore', '33647', 'fonzo',\n",
       "       '00034694', '06854', '00035306', '00035475', '06877', '33487',\n",
       "       '0699', 'walthall', 'urrabazo', '00051199', 'tabel', '06156',\n",
       "       'intermunicipal', '230c', 'tamarron', '23060', 'bepco',\n",
       "       'centerpointe', '33609', '00020867', '00020869', '23225',\n",
       "       'briarpark', '37923', '8418', '12225', '00061013', 'lifeteam',\n",
       "       '00025867', 'idleaire', '00037572', '00037708', '4109', '00013857',\n",
       "       '00013056', '8401', '00012889', '00012883', '00042945', '809w',\n",
       "       'grandbury', '00012879', '00012860', 'waldon', '207916',\n",
       "       '00012843', '06152', '00012292', '00050589', '00050725',\n",
       "       '00050742', '00050782', 'travillion', '00013073', '00013096',\n",
       "       '85254', '85048', '00038821', '85040', '00039150', '00039436',\n",
       "       'estrategy', '00039594', '00039693', '00039782', '00013686',\n",
       "       '00013436', 'dolorosa', '00013591', '00013559', 'sandbrock',\n",
       "       '39211', 'dollens', '00040778', '00013437', '00012683', '650205',\n",
       "       '00061078', '7137', '75142', '75141', '75098', '75093', '75069',\n",
       "       '100967', '75061', '75034', '75026', '75015', 'timberloch',\n",
       "       '100995', 'hdq', '74103', '19034', 'lochridge', '19001', '10155',\n",
       "       '7362', '140035', '311417', '18302', 'brocke', 'vandelinder',\n",
       "       'administaff', '5355', '5364', '75180', 'sangani', '75207', '5019',\n",
       "       'lobf', 'cintra', '76247', '00061227', '76201', '76185', '76161',\n",
       "       '76106', '76051', 'ms8656', '76049', '19603', '19486', '75209',\n",
       "       '75846', '75701', '50273', 'c27', '75559', '75450', '75265',\n",
       "       '75248', '75244', '5080', '19107', '75228', '5466', '55144',\n",
       "       '76888', '311654', '260729', '9663', '610107', '9616', '961023',\n",
       "       '10950', '684157', '26308', '149300', '619616', '94081', '26800',\n",
       "       '9415', '95814', 'tessen', '6269', '66760', 'tipro', '6310',\n",
       "       'cbbs', '15364', 'stidvent', '64153', 'twta', '65775', '9511',\n",
       "       'twnyc', '6034', '6937', 'turrieta', 'westmoor', 'boliver',\n",
       "       '92612', '14207', 'aperion', 'concreste', 'wp39', '7047', '70130',\n",
       "       'hqseo', 'jolynn', 'ph2d', '5732', '17045', '163061', '5814',\n",
       "       '16950', '59103', '10805', '9311', '16500', 'riverstone', '9764',\n",
       "       'waddy', 'lefleur', '16380', 'westgrove', '76804', '76210',\n",
       "       '76902', '78620', '79410', '4615', '79168', '4662', '79120',\n",
       "       '79106', '79105', '46835', '4691', '240130', '78861', '7880',\n",
       "       '4780', '47820', '78763', '78757', '28227', '78750', '78745',\n",
       "       '78738', '78733', '78727', '78724', '78721', '78664', '76941',\n",
       "       '78659', 'talavera', '4570', '45479', 'medrecovery', '00061425',\n",
       "       '00062269', '00062314', '00062564', '80021', 'teakell', 'trouart',\n",
       "       '11539', '4312', 'assista', '08541', 'landamerica', '08807',\n",
       "       '20058', 'gunset', '44124', 'kanik', '87109', 'giralda',\n",
       "       'coppertown', 'duple', '79912', '45102', '79905', '02117', '79901',\n",
       "       '78627', '78660', 'distirct', '13287', '30328', '77903', '77902',\n",
       "       'avex', 'boeringer', '77720', 'caremarkrx', '77553', '77536',\n",
       "       '77503', '77449', '90245', '77380', '77904', '13297', '77356',\n",
       "       '77339', '77251', '77234', 'homewise', '77099', '77074', '13333',\n",
       "       '77068', '77058', '77028', 'remcon', '87111', 'g3no', '01915',\n",
       "       '78332', '78577', '30206', '78283', 'holtcat', '78580', '78131',\n",
       "       '48265', '78401', 'kakhi', '78076', '78602', '78070', '78028',\n",
       "       'northalnd', 'kellway', '78526', 'prepress', 'multiwall',\n",
       "       'actuator', 'tradebinding', 'switchgear', 'nonclay', 'fastener',\n",
       "       'nonpackaging', 'flexographic', 'blankbook', 'lapidary', 'handsaw',\n",
       "       'nitrogenous', 'landscap', 'teleproduction', 'alkalies', 'stacker',\n",
       "       'texturizing', 'unlaminated', 'nonupholstered', 'nonfolding',\n",
       "       'equipm', 'inductor'], dtype='<U3843')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_array[tfidf_sorting[-limit:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365905"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_n = feature_array[tfidf_sorting[:limit]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(top_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['gibson', 'orwell', 'the', 'parkland', 'novel', 'maine', '1984',\n",
       "       'of', 'and', 'skinhead', 'leslie', 'to', 'is', 'george', 'alyssa',\n",
       "       'theatre', 'smartphones', 'he', 'candidate', 'astronaut',\n",
       "       'students', 'lesbian', 'gonzalez', 'in', 'predicts', 'shooting',\n",
       "       'greene', 'emma', 'survivor', 'sabattus', 'twitter', 'carson',\n",
       "       'nasa', 'insulted', 'brother', 'said', 'dystopian', 'fogg',\n",
       "       'named', 'newspeak', 'doublethink', 'by', 'that', 'winston',\n",
       "       'trained', 'activist', 'girl', 'as', 'english', 'she', 'an', 'who',\n",
       "       'gilchrist', 'line', 'icke', 'hogg', 'old', 'screens', 'it',\n",
       "       'macmillan', '57th', 'liar', 'world', 'arcadia', 'telescreen',\n",
       "       'big', 'sturridge', 'race', 'novels', 'from', 'not', 'legislature',\n",
       "       'for', 'thoughtcrime', 'republican', 'become', 'his', '17', 'bald',\n",
       "       'power', 'one', 'everyman', 'mcmahon', 'persecutes', 'being',\n",
       "       'are', 'via', 'airstrip', 'was', 'but', 'surveillance', 'calling',\n",
       "       '1949', 'what', 'individualism', 'survived', 'australia', 'wilde',\n",
       "       'its', 'year', 'lewiston', 'rights', 'out', 'entered', 'house',\n",
       "       'laughlin', 'wrote', 'ap', 'stupidity', 'this', 'our', 'florida',\n",
       "       'apathy', 'declared', 'omnipresent', 'watchful', 'public',\n",
       "       'student', 'perpetual', 'into', 'dictated', 'at', 'tartuffe',\n",
       "       'loveplay', 'birgmingham', 'pygmalion', 'organizer', 'production',\n",
       "       'have', 'jump', 'on', 'broadway', 'district', 'miser',\n",
       "       'misanthrope', 'donmar', 'hypochondriac', 'coriolanus', 'they',\n",
       "       'people', 'night', 'constitutional', 'brien', 'revolt', 'homburg',\n",
       "       'holby', 'apologizes', 'gilbert', 'jowl', 'with', 'includes',\n",
       "       'uphold', 'alchemist', 'rewritten', 'socialism', 'julia',\n",
       "       'control', 'redistributed', 'sort', 'look', 'holes', 'luminosity',\n",
       "       'shrew', 'reopened', 'concepts', 'pat', 'reportedly', 'adaptation',\n",
       "       'murmuring', 'oath', 'manipulation', 'published', 'hampstead',\n",
       "       'chichester', 'duncan', 'school', 'loesch', 'eastenders',\n",
       "       'playhouse', '2018', 'macbeth', 'dream', 'faced', 'defend',\n",
       "       'after', 'midsummer', 'high', 'birney', 'donkeys', 'wished',\n",
       "       'almeida', 'happening', 'award', 'we', 'crucible', 'room', 'state',\n",
       "       'war', 'crystal', 'inner', 'taming', 'headlong', 'romeo', 'love',\n",
       "       'impassive', 'rsc', 'only', 'audiences', 'nra', 'memory', 'turns',\n",
       "       'nomination', 'government', 'pritchard', '101', 'telescreens',\n",
       "       'apology', '70', 'will', 'party', 'arturo', 'facts', 'book',\n",
       "       'sheffield', 'tinker', 'comments', 'stands', 'liverpool', 'juliet',\n",
       "       'dropping', 'baldfaced', 'snapchatting', 'tweet', 'troutman',\n",
       "       'vic', 'olivier', 'terrifyingly', 'scarily', 'ny1', 'chosen',\n",
       "       'eryn', 'marjory', 'ball', 'has', 'facebooking', 'malevolently',\n",
       "       'since', 'stoneman', 'ui', 'democratic', 'reserved', 'superlative',\n",
       "       'functionary', 'apparitions', 'pm', 'frost', 'torturer',\n",
       "       'disorienting', 'really', 'all', 'media', 'bristol',\n",
       "       'translations', 'walking', 'firm', 'windsor', 'broadcast', '6079',\n",
       "       'magazine', 'took', 'you', 'sun', 'constitution', 'political',\n",
       "       'robert', 'foretelling', 'torre', 'dublin', 'kingston', 'merry',\n",
       "       'craves', 'knew', 'falls', 'publication', 'tailor', 'best',\n",
       "       'appropriate', 'martin', 'always', 'years', 'hand', 'copyright',\n",
       "       'tele', 'inhabits', 'adapter', 'cheek', 'their', 'subconscious',\n",
       "       'under', 'ago', 'thinking', 'outwardly', 'seat', 'watching',\n",
       "       'speaks', 'monday', 'terms', 'author', 'set', 'future', 'stage',\n",
       "       'apathetic', 'independents', 'worryingly', 'vision', 'warehouse',\n",
       "       'attack', 'wood', 'rep', 'comedy', 'protagonist', 'made', 'roma',\n",
       "       'until', 'language', 'lexicon', 'notice', 'defense', '2005',\n",
       "       'bangor', 'national', 'am', 'channeling', 'nightmares', 'caitlin',\n",
       "       'olivia', 'someone', 'matters', 'fearmongering', 'there', 'back',\n",
       "       'comrade', 'active', 'resonates', 'filed', 'arthur', 'douglas',\n",
       "       'simon', 'controlled', 'wives', 'drama', 'material', 'independent',\n",
       "       'gate', 'single', 'remarks', 'military', 'my', 'without',\n",
       "       'thrilling', 'looks', 'unsettling', 'dead', 'parallels', 'such',\n",
       "       'tragedy', 'harrowing', 'snow', 'journal', 'mantra', 'light',\n",
       "       'michael', 'inhabit', 'louder', 'away', 'muted', 'faint', 'term',\n",
       "       'curtailed', 'spy', 'desperation', 're', 'think', 'private',\n",
       "       'bestseller', 'had', 'news', 'soldier', 'prince', 'friends',\n",
       "       'about', 'smash', 'seeming', 'stephen', 'common', 'thing',\n",
       "       'resonance', 'brutally', 'performances', '45', 'realised',\n",
       "       'linear', 'spiked', 'them', 'terrifying', 'debut', 'authoritarian',\n",
       "       'review', 'meeting', 'disruptions', 'served', 'frightening',\n",
       "       'head', 'emotions', 'rebellion', 'envisioned', 'vacated', 'desk',\n",
       "       'diary', 'united', 'ignorance', 'sequence', 'time', 'written',\n",
       "       'two', 'issued', 'announced', 'catalyst', 'or', 'mrs', 'errors',\n",
       "       'associated', 'literary', 'so', 'leftist', 'course', 'lauded',\n",
       "       'like', 'many', 'nineteen', 'explores', 'co', 'film', 'account',\n",
       "       'scandal', 'criticized', 'staging', '100', 'constant', 'alternate',\n",
       "       'off', 'amazing', 'invasive', 'congressmen', 'corey', 'through',\n",
       "       'plenty', 'sticks', 'hung', 'dana', 'when', 'politicians',\n",
       "       'relevance', 'masses', 'touch', 'enough', 'judges', 'resist',\n",
       "       'march', 'states', 'reed', 'because', 'theater', 'also', 'ever',\n",
       "       'motivation', 'now', 'torture', 'no', 'very', 'celebrated', 'than',\n",
       "       'graphic', 'themes', 'climbed', 'other', 'tale', 'system', 'makas',\n",
       "       '2017', 'how', 'take', 'slavery', 'coincidence', 'apologized',\n",
       "       'eighty', 'fire', 'rose', 'something', 'saying'], dtype='<U3843')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n == stop_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_n = feature_array[tfidf_sorting[:limit]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
