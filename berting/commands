python run_lang_model.py \
--data_dir=/home/billwuyu_gmail_com/fakenews/data/train \
--model_type=roberta \
--model_name_or_path=roberta-large \
--task_name=fn \
--output_dir=/home/billwuyu_gmail_com/results/rbta_lrg_cdn_glb_full \
--max_seq_length=512 \
--do_train \
--evaluate_during_training \
--gradient_accumulation_steps=11 \
--overwrite_output_dir \
--per_gpu_train_batch_size=3 \
--per_gpu_eval_batch_size=3 \
--num_train_epochs=3.0 \
--logging_steps=100 \
--save_steps=100 \
--learning_rate=2e-5 \
--weight_decay=0.1 \
--run_name=runs/rbta_lrg_cdn_glb_full

python run_lang_model.py \
--data_dir=/home/billwuyu_gmail_com/fakenews/data/train \
--model_type=roberta \
--model_name_or_path=/home/billwuyu_gmail_com/results/multi_article_roberta_v2split \
--task_name=fn \
--output_dir=/home/billwuyu_gmail_com/results/multi_article_roberta_v2split \
--max_seq_length=512 \
--do_test \
--per_gpu_eval_batch_size=1 \

python run_glue.py \
  --task_name=sts-b \
  --do_train \
  --do_eval \
  --do_lower_case \
  --model_type=bert \
  --data_dir=/home/billwuyu_gmail_com/glue_data/STS-B \
  --model_name_or_path=bert-base-uncased \
  --max_seq_length=128 \
  --per_gpu_train_batch_size=32 \
  --per_gpu_eval_batch_size=32 \
  --learning_rate=2e-5 \
  --num_train_epochs=3.0 \
  --save_steps=100 \
  --output_dir=/home/billwuyu_gmail_com/results/bert_base_uncased_sts-b \
  --cache_dir=/home/billwuyu_gmail_com/pretrained_models \


docker run --gpus all -v /home/billwuyu_gmail_com/dataset/:/usr/local/dataset/:ro --name fakenews ava

nvidia-docker run \
-v /home/billwuyu_gmail_com/dataset/:/usr/local/dataset/:ro \
--name submission_container -e LANG=C.UTF-8 --net=none ava

docker build -t ava .
docker save -o ava.tar ava
docker load --input ava3.tar
docker cp submission_container:/usr/local/predictions.txt .

python do_inference.py \
--data_dir=/home/billwuyu_gmail_com/dataset \
--model_dir=/home/billwuyu_gmail_com/docker_submission2 \
--output_file_path=/home/billwuyu_gmail_com/docker_submission2/predictions.txt \


12/02/2019 03:53:49 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /home/billwuyu_gmail_com/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.e
f00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
12/02/2019 03:53:49 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /home/billwuyu_gmail_com/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.7
0bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda