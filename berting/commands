python run_lang_model.py \
--data_dir=/home/ubuntu/fakenews/data/train \
--model_type=bert \
--model_name_or_path=bert-base-cased \
--task_name=fn \
--output_dir=/home/ubuntu/results/top20_sorted_multi_article_2e-5 \
--max_seq_length=512 \
--do_train \
--evaluate_during_training \
--gradient_accumulation_steps=32 \
--cache_dir=/home/ubuntu/pretrained_models \
--overwrite_output_dir \
--per_gpu_train_batch_size=1 \
--per_gpu_eval_batch_size=1 \
--num_train_epochs=3.0 \
--logging_steps=100 \
--save_steps=500 \
--learning_rate=2e-5 \
--run_name=runs/top20_sorted_multi_article_2e-5

python run_lang_model.py \
--data_dir=/home/ubuntu/fakenews/data/train \
--model_type=bert \
--model_name_or_path=/home/ubuntu/results/condensed_v3_claimant_cased_2e-5 \
--task_name=fn \
--output_dir=/home/ubuntu/results/condensed_v3_claimant_cased_2e-5 \
--max_seq_length=512 \
--do_test \
--per_gpu_eval_batch_size=128 \

python run_glue.py \
  --task_name=sts-b \
  --do_train \
  --do_eval \
  --do_lower_case \
  --model_type=bert \
  --data_dir=/home/ubuntu/glue_data/STS-B \
  --model_name_or_path=bert-base-uncased \
  --max_seq_length=128 \
  --per_gpu_train_batch_size=32 \
  --per_gpu_eval_batch_size=32 \
  --learning_rate=2e-5 \
  --num_train_epochs=3.0 \
  --save_steps=100 \
  --output_dir=/home/ubuntu/results/bert_base_uncased_sts-b \
  --cache_dir=/home/ubuntu/pretrained_models \


docker run --gpus all -v /home/ubuntu/dataset/:/usr/local/dataset/:ro --name fakenews ava

python do_inference.py \
--data_dir=/home/ubuntu/dataset \
--model_dir=/home/ubuntu/docker_submission \
--output_file_path=/home/ubuntu/dataset/predictions.txt \
